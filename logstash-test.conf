input {
 stdin { }
    jdbc {
		#数据库连接地址
        jdbc_connection_string => "jdbc:mysql://localhost:3306/yjgl"
		#用户名
        jdbc_user => "root"
		#密码
        jdbc_password => "123456"
		
        jdbc_driver_library => "C:/Program Files (x86)/MySQL/Connector.J 5.1/mysql-connector-java-5.1.38-bin.jar"
		# driver配置
        jdbc_driver_class => "com.mysql.jdbc.Driver"
         
		#是否记录上一次运行
		record_last_run =>"true"
		
		#是否使用列名
		use_column_value => "false"
		
		#上一次运行的记录地址
		last_run_metadata_path => "/run_metadata/my_info"
		
		jdbc_paging_enabled => "true"
        jdbc_page_size => "50000"
		
		# 语句执行地址
        statement => "SELECT * FROM ac_report"
		# statement_filepath => "/logstash_jdbc/jdbc.sql"
        schedule => "* * * * *"
		
		# type属性用于多表导入时的 区分点
		# type => "tstype"
    }
	#导入多表时重复jdbc块即可，使用type进行区分
	#jdbc{
	#	type => "test"
	#}
 }
  
 filter {
	  json {
	   source => "message"
	   remove_field => ["message"]
	  }
	  #mutate {
	  # convert => [ "publish_time", "string" ]
	  # }
	  #grok {
	  # match => { "message" => "%{COMBINEDAPACHELOG}" }
	  # match => { "message" => "test" }
	  #}
	  #date {
	  # match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
	  #}
 }
 
 output {
    stdout {
        codec => json_lines
    }
    elasticsearch {
        hosts => "localhost:9200"
        index => "learn"
		# document_type属性即将过时，
        document_type => "content"
        document_id => "%{id}"
    }
	#kafka {
    #  bootstrap_servers => "x.x.x.x:9092,x.x.x.x:9092"
     # topic_id => "logstashlog"
    #}
	
	#根据不同的type进行判断，导入到对应的index中即可
	#if [type]=="cxxarticle_info" {
	 # elasticsearch {
	 # hosts => "localhost:9200"
	
	 # index => "test1_index"
	
	 # document_id => "%{id}"
	  #}
	 #}
}